{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issues with student loans appear to involve problems related to lender or servicer misconduct, such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and difficulties with handling payments. Additionally, issues like incorrect information on credit reports, confusion caused by loan transfers without proper notification, and problems with repayment plans (including disputes over interest increases and improper handling of loan forgiveness or discharge) are frequently mentioned.\\n\\nIn summary, the most common issues involve:\\n\\n- Errors in loan balances and account information\\n- Misapplication or mishandling of payments\\n- Lack of transparency and notification about loan transfers and changes\\n- Problems with repayment plans and interest rate increases\\n- Disputes over incorrect reporting and loan management\\n\\nPlease note that these reflect the recurring problems highlighted in the complaints data, indicating that administrative errors and miscommunications are among the most prevalent issues faced by borrowers.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint (row 441) was marked as \"Not timely,\" indicating that the response or resolution took longer than expected. Additionally, multiple complaints (like row 67 and row 816) mention delays or that the issue remains unresolved after a significant period, such as over a year or nearly 18 months, suggesting these were not handled within a timely timeframe.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for several reasons, including:\\n\\n1. **Accumulation of interest during deferment or forbearance:** Borrowers were often given options like forbearance or deferment, but interest continued to accrue, increasing the total amount owed and making repayment more difficult over time.\\n\\n2. **Unmanageable repayment options:** Lowering monthly payments to make them more affordable extended the repayment period and increased overall interest, creating a cycle where borrowers could not fully pay off their loans.\\n\\n3. **Lack of clear information and poor communication:** Borrowers frequently reported not being adequately informed about loan terms, repayment schedules, loan transfers, or changes in servicers. This lack of transparency led to missed payments, credit issues, or unawareness of when repayment was expected to resume.\\n\\n4. **Financial hardship and stagnant wages:** Many borrowers faced economic challenges such as stagnating wages, job loss, or long-term unemployment, which made consistent repayment impossible despite their efforts.\\n\\n5. **Inadequate or inappropriate handling of payments and loan management:** Issues like difficulty applying extra payments toward principal, automatic placement into forbearance, or mismanagement during loan transfers contributed to ongoing debt and inability to reduce loan balances.\\n\\n6. **Mismanagement and lack of support from loan servicers:** Some borrowers experienced problems with loan servicer communications, including being unaware of repayment obligations, unexpected delinquencies, or incorrect reporting to credit bureaus.\\n\\n7. **High interest rates and long-term debt accumulation:** Years of high interest rates, compounded over time through multiple forbearances or long-term payment plans, caused balances to grow despite ongoing payments.\\n\\nOverall, the combination of systemic issues like poor communication, financial hardship, and loan management practices contributed significantly to borrowers' inability to successfully pay back their loans.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be dealing with the lender or servicer, including problems such as unhelpful or dishonest communication, difficulty in obtaining accurate information about loan balances, interest calculations, or repayment terms, and issues with loan servicing errors. Multiple complaints highlight problems like being given bad or confusing information, issues with payment application, and disputes over fees or loan details.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints in the context received timely responses from the companies, as indicated by the \"Timely response?\" field being \"Yes\" for each complaint. Therefore, no complaints appear to have gone unhandled in a timely manner.'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People fail to pay back their loans for various reasons, including problems with payment plans, difficulties in communication with lenders or servicers, and issues related to mismanagement or errors by the loan servicers. In some cases, borrowers have experienced challenges due to the servicers steering them into incorrect forbearance options, having their automatic payments unenrolled without proper notification, or facing repeated reversal of payments despite making timely payments. Additionally, some borrowers report being unaware of transfer of their loans to new servicers, lack of clear communication about their account status, or being falsely reported as overdue, all of which can hinder their ability to successfully repay loans.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "#### âœ… Answer:\n",
        "\n",
        "**Example Query:** \"What is the APR for Chase Freedom card?\"\n",
        "\n",
        "**Why BM25 is better:**\n",
        "- **Exact keyword matching**: BM25 will precisely match \"Chase Freedom\" and \"APR\" terms\n",
        "- **Specific product names**: Embeddings might confuse \"Chase Freedom\" with other Chase cards or general freedom concepts\n",
        "- **Technical terms**: \"APR\" is a specific financial term that BM25 handles better than semantic embeddings\n",
        "- **Domain-specific vocabulary**: Financial products have precise naming conventions that benefit from exact matching\n",
        "\n",
        "**Justification:** BM25 excels at finding documents containing the exact product name and technical term, while embeddings might retrieve documents about general credit card information or Chase banking services that don't specifically mention the Freedom card's APR.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to involve problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and improper handling or transfer of loans. Additionally, issues such as receiving incorrect or bad information, lack of clear communication, and disputes over account discrepancies are prevalent.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, some complaints indicate delays in handling: \\n- One complaint has been open since over a year with no resolution, specifically regarding a request for account review and discharge due to violations. \\n- Another complaint, related to a previous issue not being addressed, has been ongoing for over 2-3 weeks.\\n- The complaint about payments not appearing on the account was submitted on May 2, 2025, but there is no indication it was resolved promptly.\\n\\nWhile the responses from companies in these cases are marked as \"Closed with explanation\" and responses are considered timely, the extended duration of some issues suggests that certain complaints were not handled in a timely manner from the consumerâ€™s perspective.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, including:\\n\\n1. Lack of awareness and understanding: Borrowers often did not realize they had to repay their loans or were not properly informed by financial aid officers, leading to surprise and confusion about repayment obligations.\\n\\n2. Poor communication and notification: Borrowers reported not being adequately notified about loan transferences, due dates, or the start of repayment, which contributed to missed payments or late payments.\\n\\n3. Financial hardship and inability to afford payments: Many borrowers found the monthly payments unaffordable, especially when interest continued to accrue during forbearance or deferment, making it difficult to pay down the principal.\\n\\n4. Accumulation of interest and increasing balances: For some, interest accumulated during forbearance or when payments were reduced, causing balances to grow over time despite ongoing payments, further complicating repayment efforts.\\n\\n5. Unfavorable repayment options and loan terms: Borrowers felt that available options like forbearance or deferment extended the repayment period and increased total interest paid, which discouraged or prevented full repayment.\\n\\n6. Administrative issues and errors: Some faced issues like incorrect account information, lack of documentation, or unauthorized transfers of their loans, which impeded their ability to make or track payments properly.\\n\\n7. Limited options for manageable repayment: Borrowers expressed that the options offered did not suit their financial situations, and that interest accumulation and stagnant wages made it unrealistic to pay off their loans under the current terms.\\n\\nThese factors combined contributed to difficulties in repaying loans and, in some cases, led to default or continued growing debt despite payments.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, the most common issues with loans tend to involve mismanagement and inaccurate information, including errors in loan balances, incorrect reporting of account status, and mishandling of payments such as misapplied payments, wrongful denials of repayment plans, and incorrect loan classifications. Many complaints also highlight problems with loan servicing companies failing to provide proper documentation, improper handling of loan deferments, and issues related to loan transfer and sale, which can lead to confusion and errors in loan account status and balances.\\n\\nIn summary, the most common issue appears to be **mismanagement and errors by loan servicers, leading to incorrect loan information, mishandled payments, and inadequate communication with borrowers**.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, yes, some complaints did not get handled in a timely manner. Specifically, there are instances indicating delays:\\n\\n- On complaint ID **12654977** (submitted to MOHELA), the response was **\"No\"** for timely response, and it states that despite efforts, the issue remained unresolved with over 3 hours of wait times when calling customer service, and messages sent through the inbox went unanswered. Additionally, the complaint was marked \"Closed with explanation,\" implying the issue was not resolved promptly.\\n\\n- Similarly, for complaint ID **12973003** (submitted to EdFinancial), the response was **\"Yes\"** for timely response, yet the complaint indicates that the issue persisted over 2-3 weeks and was not resolved, suggesting a delay or inadequate handling.\\n\\nIn multiple cases, consumers reported prolonged unresolved issues, long wait times, or lack of follow-up, which indicates that not all complaints were handled in a timely manner.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of misunderstandings, lack of clear information, systemic issues, and hardships such as financial difficulties and mismanagement. Specific reasons include:\\n\\n1. Accumulation of interest during forbearance or deferment, which made loans unpayable or extended the repayment period.\\n2. Misleading or inadequate information from servicers about repayment options like income-driven plans or rehabilitation programs, leading borrowers to be steered into long-term forbearances or aggressive consolidation practices.\\n3. High interest rates and compounded interest working against borrowers, increasing the total debt over time.\\n4. Systemic failures and misconduct by servicers, including errors in reporting, mishandling of accounts, and failure to communicate important deadlines or options.\\n5. Financial hardships, unemployment, low wages, or other personal circumstances making it difficult for borrowers to afford payments.\\n6. Confusing or opaque communication about delinquency, account status, or the consequences of missed payments.\\n7. Borrowers feeling misled, coerced, or not adequately informed about alternative repayment options, leading to unmanageable debt.\\n8. Technical issues and mishandling of accounts that further complicated repayment efforts.\\n\\nOverall, many borrowers struggled not because of irresponsibility, but due to systemic issues, misleading practices, and financial hardships that made repayment schemes unfeasible.'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "#### âœ… Answer :\n",
        "\n",
        "**Core Mechanism:**\n",
        "- Takes original query â†’ LLM generates multiple variations â†’ Retrieves documents for each â†’ Combines unique results\n",
        "\n",
        "**This improves recall due to:**\n",
        "\n",
        "1. **Query Diversity**: Different ways to ask the same question find different relevant documents\n",
        "   - Original: \"What is the most common issue with loans?\"\n",
        "   - Reformulations: \"What problems do people have with loans?\", \"What are the main loan complaints?\", \"What issues arise with student loans?\"\n",
        "\n",
        "2. **Vocabulary Coverage**: Captures synonyms and alternative phrasings\n",
        "   - \"Payment problems\" vs \"billing issues\" vs \"repayment difficulties\"\n",
        "\n",
        "3. **Semantic Variations**: Different angles of the same question retrieve different document subsets\n",
        "   - Some documents might use \"servicer\" instead of \"lender\"\n",
        "   - Some might mention \"forbearance\" instead of \"payment pause\"\n",
        "\n",
        "**Example from our dataset:**\n",
        "- Original: \"Why did people fail to pay back their loans?\"\n",
        "- Reformulations might include: \"What causes loan default?\", \"Why do borrowers struggle with repayment?\", \"What prevents successful loan payment?\"\n",
        "\n",
        "**Result:** Higher recall because you're searching with multiple query formulations, each potentially finding different relevant documents that the original query might miss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issues with loans tend to involve misconduct or errors related to loan servicing. Specific common issues include: \\n\\n- Errors in loan balances\\n- Misapplication of payments\\n- Wrongful denials of payment plans\\n- Discrepancies in loan balances and interest rates\\n- Unfair or deceptive practices by loan servicers\\n- Illegal credit reporting related to unverified or questionable debts\\n\\nOverall, errors in the administration and reporting of loans, as well as issues with loan servicer misconduct, appear to be the most prevalent problems mentioned.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, the complaints regarding issues with student loan servicing and payment processing (such as those submitted to MOHELA and Aidvantage) were marked as \"Timely response?\": \"No.\" Additionally, the complainant states that they have not received responses within the expected timeframes, and in some cases, there were significant delays or no response at all. Therefore, there were complaints that were not handled promptly.'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People often failed to pay back their loans due to a variety of issues highlighted in the complaints. These include:\\n\\n- Lack of clear communication from loan servicers about payment schedules, especially prior to the end of grace periods.\\n- Misrepresentations or lack of transparency about the long-term financial consequences of taking out loans.\\n- Financial hardship or severe economic difficulties, such as unemployment or health issues, making it impossible to make payments.\\n- Problems related to the management and legitimacy of the debt, such as failure to verify debt validity or improper reporting to credit bureaus.\\n- Difficulties arising from educational institutions' misconduct or closure, which impacted graduates' ability to secure employment and repay loans.\\n- Issues with the transfer of loans between different agencies or servicers, leading to missed notifications and missed payments.\\n\\nOverall, failure to pay back loans often resulted from administrative errors, inadequate support and communication, or genuine financial hardship faced by borrowers.\""
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issue with loans appears to be problems related to \"Dealing with your lender or servicer,\" particularly issues such as:\\n\\n- Errors in loan balances and misapplied payments\\n- Wrongful denials of repayment plans or issues with interest accrual\\n- Inaccurate or conflicting information about loan status and balances\\n- Lack of proper communication or notices from loan servicers\\n- Problems with loan transfers and misconduct\\n- Inaccurate credit reporting and negative impacts on credit scores\\n\\nWhile there are various specific sub-issues, a recurring theme is maladministration or misconduct by loan servicers, leading to incorrect loan information, unexpected interest accumulation, or inadequate communication. Thus, the most common issue is problems stemming from the handling and communication by lenders or servicers.'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, some complaints did not get handled in a timely manner. Specifically, at least two complaintsâ€”Complaint ID 12668396 and Complaint ID 12739706â€”were marked as \"No\" or \"Late\" responses, indicating they were not responded to within the expected timeframe. Additionally, several complaints noted delays or failures to respond promptly, suggesting that handling was not always timely.'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, people failed to pay back their loans primarily due to a combination of factors including:\\n\\n1. Lack of clear or adequate communication from loan servicers about payment obligations, due dates, and available options, leading to confusion and unintentional delinquencies.\\n2. Suspension or transfer of loan accounts without proper notification, which caused borrowers to be unaware of the start or resumption of payments.\\n3. Difficulties in accessing or managing online accounts and a lack of documentation, resulting in errors in payment application and balances.\\n4. Deceptive or coercive practices such as long-term forbearance steering, which prevented borrowers from entering income-driven repayment or rehabilitation plans that could have made repayment more manageable.\\n5. Errors in reporting, including incorrect delinquency status or missed payments, which adversely impacted credit scores.\\n6. Systemic mismanagement, including failure to provide transparent details about interest accrual, balance calculations, and ownership transfers, which increased the debt burden over time.\\n7. Hardship situations like unemployment, health issues, or financial hardship that made monthly payments unfeasible, especially when coupled with aggressive collection efforts and insufficient support.\\n\\nOverall, the failure to repay was not solely due to irresponsibility, but was often compounded by systemic issues, malicious practices, and inadequate communication by servicers, which created circumstances where borrowers could not keep up with their loans despite their efforts.\\n\\nIf you have any more questions or need further assistance, please let me know!'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be problems related to loan servicing and misinformation. This includes issues such as struggling to make payments, incorrect or delayed information about account status or payment plans, disputes over the legitimacy or accuracy of reported debt, and difficulty in communication with loan servicers. Many complaints highlight errors or discrepancies in how loans are handled, which can cause stress, financial impact, and violations of legal protections.'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that several complaints were marked as \"Closed with explanation\" and have statements indicating that responses were timely (\"Yes\"), which suggests they were handled within the expected timeframe. However, the complaint regarding Nelnet (ID 13331376) specifically notes that despite acknowledgment and repeated letters, the company never responded to the complaint, which implies it was not handled in a timely manner. \\n\\nTherefore, yes, there was at least one complaint that did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, people failed to pay back their loans primarily due to issues such as miscommunication, lack of transparency from loan servicers, technical difficulties, and delays or problems with payment processing. Some individuals faced difficulties logging into their accounts, receiving inadequate assistance, or encountering stalling tactics by loan servicers that discouraged ongoing payment efforts. Additionally, there were cases involving improper reporting, disputes over the legitimacy of certain debts, and alleged breaches of privacy laws, all of which contributed to complications in repayment.'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "#### âœ… Answer:\n",
        "\n",
        "In this scenario, there are several **problems** with standard semantic chunking:\n",
        "\n",
        "1. **Over-chunking**: Short, repetitive sentences will have very similar embeddings, causing the algorithm to create many tiny, redundant chunks\n",
        "2. **Poor semantic boundaries**: FAQs often have similar semantic content across different questions, making it hard to find natural breakpoints\n",
        "3. **Reduced retrieval quality**: Tiny chunks lose context and become less useful for retrieval\n",
        "\n",
        "**Adjustments Needed:**\n",
        "\n",
        "1. **Increase minimum chunk size**: Set a higher minimum threshold to prevent over-fragmentation\n",
        "2. **Use different thresholding**: Switch from `percentile` to `standard_deviation` or `interquartile` for better handling of repetitive content\n",
        "3. **Pre-process content**: Group similar FAQs before chunking to reduce redundancy\n",
        "4. **Adjust similarity thresholds**: Use higher similarity thresholds to keep related content together\n",
        "5. **Consider hierarchical chunking**: Create larger chunks first, then sub-chunk only when necessary\n",
        "6. **Hybrid Strategy**: Use semantic chunking for longer, diverse content and use fixed-size chunking for repetitive FAQ sections\n",
        "\n",
        "**Example Adjustment:**\n",
        "```python\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"standard_deviation\",  # Instead of percentile\n",
        "    min_chunk_size=100,  # Prevent tiny chunks\n",
        "    similarity_threshold=0.8  # Higher threshold for repetitive content\n",
        ")\n",
        "```\n",
        "\n",
        "**Result:** Better handling of FAQ-style content by creating more meaningful, larger chunks that preserve context while avoiding redundancy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### âœ… Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LangSmith configured for evaluation\n",
            "ðŸ“Š Dataset: Advanced-Retrieval-Evaluation-20250729-010657\n",
            "ï¿½ï¿½ Dataset URL: https://smith.langchain.com/datasets/cfd03fa7-cbef-4680-acca-b7336570fc6e\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 1. SETUP LANGSMITH DATASET\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "from getpass import getpass \n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from langsmith import Client\n",
        "\n",
        "# Setup LangSmith API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Please enter your LANGCHAIN API key!\")\n",
        "\n",
        "# Create LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Create dataset for evaluation\n",
        "dataset_name = f\"Advanced-Retrieval-Evaluation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Advanced Retrieval Methods Evaluation Dataset\"\n",
        ")\n",
        "\n",
        "print(\"âœ… LangSmith configured for evaluation\")\n",
        "print(f\"ðŸ“Š Dataset: {dataset_name}\")\n",
        "print(f\"ï¿½ï¿½ Dataset URL: https://smith.langchain.com/datasets/{langsmith_dataset.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Loaded 269 PDFs + 825 complaints = 1094 total documents\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 2. LOAD DOCUMENTS\n",
        "# ============================================================================\n",
        "\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "# Load PDFs from data/ (like reference notebook)\n",
        "path = \"data/\"\n",
        "pdf_loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "pdf_docs = pdf_loader.load()\n",
        "\n",
        "# Combine with existing loan complaint data\n",
        "all_documents = pdf_docs + loan_complaint_data\n",
        "\n",
        "print(f\"ðŸ“‚ Loaded {len(pdf_docs)} PDFs + {len(loan_complaint_data)} complaints = {len(all_documents)} total documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”„ Generating synthetic questions...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95c8f7b5f2da498f82ca5d5cfebebc2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e74fe7009064be2893cd52006f9a4cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85e0dbe7975941f4beff5570d0513f91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '4cec46'. Skipping!\n",
            "Property 'summary' already exists in node 'b4244f'. Skipping!\n",
            "Property 'summary' already exists in node 'e54db4'. Skipping!\n",
            "Property 'summary' already exists in node '8540a4'. Skipping!\n",
            "Property 'summary' already exists in node '3a8e7d'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f190c98524d24e57bc360f038a179fb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dd3d6816ead44498d9ef5cf5e17d5ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node 'b4244f'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '3a8e7d'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '4cec46'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'e54db4'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '8540a4'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5cb548e3d194d168a9fe668613b6e2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0442968cf32547b18ab81161456f8b17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad45197dc0f348f5b28f32e51d601640",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3372604b51954d339dd48769def2243d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Generated 12 synthetic questions\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What obligations does Maximus have under the F...</td>\n",
              "      <td>[XXXX XXXX XXXX XXXX. XXXX XXXX XXXX XXXX XXXX...</td>\n",
              "      <td>According to the provided context, when a cons...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wut dos 15 U.S.C. 1681i requir when disputin s...</td>\n",
              "      <td>[XXXX XXXX XXXX XXXX. XXXX XXXX XXXX XXXX XXXX...</td>\n",
              "      <td>Under 15 U.S.C. 1681i(a)(1)(A), a credit repor...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is Aidvantage as mentioned in the summary...</td>\n",
              "      <td>[Failure to Comply Will Result in Further Acti...</td>\n",
              "      <td>Aidvantage is listed as the name of an account...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What does FCRA refer to in the context of stud...</td>\n",
              "      <td>[Failure to Comply Will Result in Further Acti...</td>\n",
              "      <td>FCRA refers to the legal framework under which...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the formal dispute of student loan ac...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>The formal dispute of student loan accounts is...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what happen if they dont fix my dispute of stu...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>if they dont fix your dispute of student loan ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the formal dispute of student loan ac...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>The formal dispute of student loan accounts be...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the formal dispute of student loan ac...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>The formal dispute of student loan accounts qu...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How do the requirements of 15 U.S.C. 1681e reg...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>Under 15 U.S.C. 1681e(b), a credit reporting a...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does the formal dispute letter regarding u...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>The formal dispute letter cites 15 U.S.C. 1681...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does 15 U.S.C. 1681s-2 relate to the oblig...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>Under 15 U.S.C. 1681s-2, information furnisher...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How does the Fair Credit Reporting Act, specif...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...</td>\n",
              "      <td>The Fair Credit Reporting Act (FCRA), as refer...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What obligations does Maximus have under the F...   \n",
              "1   Wut dos 15 U.S.C. 1681i requir when disputin s...   \n",
              "2   What is Aidvantage as mentioned in the summary...   \n",
              "3   What does FCRA refer to in the context of stud...   \n",
              "4   How does the formal dispute of student loan ac...   \n",
              "5   what happen if they dont fix my dispute of stu...   \n",
              "6   How does the formal dispute of student loan ac...   \n",
              "7   How does the formal dispute of student loan ac...   \n",
              "8   How do the requirements of 15 U.S.C. 1681e reg...   \n",
              "9   How does the formal dispute letter regarding u...   \n",
              "10  How does 15 U.S.C. 1681s-2 relate to the oblig...   \n",
              "11  How does the Fair Credit Reporting Act, specif...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [XXXX XXXX XXXX XXXX. XXXX XXXX XXXX XXXX XXXX...   \n",
              "1   [XXXX XXXX XXXX XXXX. XXXX XXXX XXXX XXXX XXXX...   \n",
              "2   [Failure to Comply Will Result in Further Acti...   \n",
              "3   [Failure to Comply Will Result in Further Acti...   \n",
              "4   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "5   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "6   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "7   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "8   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "9   [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "10  [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "11  [<1-hop>\\n\\nXXXX XXXX XXXX XXXX. XXXX XXXX XXX...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   According to the provided context, when a cons...   \n",
              "1   Under 15 U.S.C. 1681i(a)(1)(A), a credit repor...   \n",
              "2   Aidvantage is listed as the name of an account...   \n",
              "3   FCRA refers to the legal framework under which...   \n",
              "4   The formal dispute of student loan accounts is...   \n",
              "5   if they dont fix your dispute of student loan ...   \n",
              "6   The formal dispute of student loan accounts be...   \n",
              "7   The formal dispute of student loan accounts qu...   \n",
              "8   Under 15 U.S.C. 1681e(b), a credit reporting a...   \n",
              "9   The formal dispute letter cites 15 U.S.C. 1681...   \n",
              "10  Under 15 U.S.C. 1681s-2, information furnisher...   \n",
              "11  The Fair Credit Reporting Act (FCRA), as refer...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3. GENERATE SYNTHETIC QUESTIONS WITH SDG\n",
        "# ============================================================================\n",
        "\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "print(\"\\nðŸ”„ Generating synthetic questions...\")\n",
        "\n",
        "# Sample 50 documents for efficiency\n",
        "import random\n",
        "sample_docs = random.sample(all_documents, min(50, len(all_documents)))\n",
        "\n",
        "# Initialize Ragas generator (exact same pattern as reference notebook)\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "# Generate testset (exact same pattern as reference notebook)\n",
        "testset = generator.generate_with_langchain_docs(sample_docs[:20], testset_size=10)\n",
        "\n",
        "# Extract questions\n",
        "test_questions = list(testset.to_pandas()['user_input'])\n",
        "print(f\"âœ… Generated {len(test_questions)} synthetic questions\")\n",
        "\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ” Evaluating retrievers...\n",
            "\n",
            "ðŸ“Š Evaluating naive...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9fe94f6176c44eb8f95ec3f643d3120",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… naive: Precision=1.000\n",
            "\n",
            "ðŸ“Š Evaluating bm25...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ee97fecd864415db55806fdfee5f0bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… bm25: Precision=0.955\n",
            "\n",
            "ðŸ“Š Evaluating multi_query...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b67747669c514876964c32a98b3d7f1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… multi_query: Precision=1.000\n",
            "\n",
            "ðŸ“Š Evaluating parent_doc...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6086156589354f0b9dfbe6cccd1f52b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… parent_doc: Precision=1.000\n",
            "\n",
            "ðŸ“Š Evaluating compression...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25f9be59f080478e879b915679c8a079",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… compression: Precision=1.000\n",
            "\n",
            "ðŸ“Š Evaluating ensemble...\n",
            "  âœ… Question 1/8 completed\n",
            "  âœ… Question 2/8 completed\n",
            "  âœ… Question 3/8 completed\n",
            "  âœ… Question 4/8 completed\n",
            "  âœ… Question 5/8 completed\n",
            "  âœ… Question 6/8 completed\n",
            "  âœ… Question 7/8 completed\n",
            "  âœ… Question 8/8 completed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92bf83110c9d4e08be1dec52275ecdc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ensemble: Precision=0.990\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4. EVALUATE RETRIEVERS\n",
        "# ============================================================================\n",
        "\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import context_precision, faithfulness, answer_relevancy, context_recall\n",
        "from datasets import Dataset\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "print(\"\\nðŸ” Evaluating retrievers...\")\n",
        "\n",
        "# Define retrievers\n",
        "retrievers = {\n",
        "    \"naive\": naive_retriever,\n",
        "    \"bm25\": bm25_retriever,\n",
        "    \"multi_query\": multi_query_retriever,\n",
        "    \"parent_doc\": parent_document_retriever,\n",
        "    \"compression\": compression_retriever,\n",
        "    \"ensemble\": ensemble_retriever\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, retriever in retrievers.items():\n",
        "    print(f\"\\nðŸ“Š Evaluating {name}...\")\n",
        "    \n",
        "    eval_data = []\n",
        "    \n",
        "    # Use first 8 questions for each retriever\n",
        "    for i, question in enumerate(test_questions[:8]):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Get contexts\n",
        "        contexts = retriever.invoke(question)\n",
        "        context_text = \"\\n\".join([doc.page_content for doc in contexts])\n",
        "        \n",
        "        # Generate answer with cost tracking\n",
        "        prompt = f\"Question: {question}\\n\\nContext: {context_text}\\n\\nAnswer:\"\n",
        "        \n",
        "        with get_openai_callback() as cb:\n",
        "            response = chat_model.invoke(prompt)\n",
        "        \n",
        "        # Create example for LangSmith dataset\n",
        "        example = client.create_example(\n",
        "            inputs={\"question\": question},\n",
        "            outputs={\"answer\": response.content},\n",
        "            dataset_id=langsmith_dataset.id\n",
        "        )\n",
        "        \n",
        "        eval_data.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": response.content,\n",
        "            \"contexts\": [doc.page_content for doc in contexts],\n",
        "            \"ground_truth\": response.content,\n",
        "            \"latency\": time.time() - start_time,\n",
        "            \"cost\": cb.total_cost\n",
        "        })\n",
        "        \n",
        "        print(f\"  âœ… Question {i+1}/8 completed\")\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"question\": [d[\"question\"] for d in eval_data],\n",
        "        \"answer\": [d[\"answer\"] for d in eval_data],\n",
        "        \"contexts\": [d[\"contexts\"] for d in eval_data],\n",
        "        \"ground_truth\": [d[\"ground_truth\"] for d in eval_data]\n",
        "    })\n",
        "    \n",
        "    # Evaluate with Ragas\n",
        "    ragas_result = evaluate(dataset, metrics=[context_precision, faithfulness, answer_relevancy, context_recall])\n",
        "    \n",
        "    # Helper function to extract metric values (handles both list and single value formats)\n",
        "    def extract_metric_value(metric_result):\n",
        "        if isinstance(metric_result, list):\n",
        "            return sum(metric_result) / len(metric_result)  # Calculate mean\n",
        "        return metric_result\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"context_precision\": extract_metric_value(ragas_result[\"context_precision\"]),\n",
        "        \"faithfulness\": extract_metric_value(ragas_result[\"faithfulness\"]),\n",
        "        \"answer_relevancy\": extract_metric_value(ragas_result[\"answer_relevancy\"]),\n",
        "        \"context_recall\": extract_metric_value(ragas_result[\"context_recall\"]),\n",
        "        \"avg_latency\": sum(d[\"latency\"] for d in eval_data) / len(eval_data),\n",
        "        \"total_cost\": sum(d[\"cost\"] for d in eval_data)\n",
        "    }\n",
        "    \n",
        "    print(f\"âœ… {name}: Precision={results[name]['context_precision']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ“Š RESULTS\n",
            "================================================================================\n",
            "  Retriever Precision Faithfulness Relevancy Recall Latency (s) Cost ($)\n",
            "      Naive     1.000        0.920     0.822  0.978        6.65   0.0036\n",
            "       Bm25     0.955        0.919     0.817  0.984        4.55   0.0025\n",
            "Multi Query     1.000        0.940     0.943  0.960        8.79   0.0052\n",
            " Parent Doc     1.000        0.819     0.940  1.000        6.36   0.0026\n",
            "Compression     1.000        0.972     0.829  1.000        7.48   0.0020\n",
            "   Ensemble     0.990        0.844     0.819  0.986        9.95   0.0071\n",
            "\n",
            "ðŸ† RANKING:\n",
            "1. Multi Query: 0.963\n",
            "2. Compression: 0.950\n",
            "3. Parent Doc: 0.940\n",
            "4. Naive: 0.931\n",
            "5. Bm25: 0.917\n",
            "6. Ensemble: 0.910\n",
            "\n",
            "ðŸ’¡ Best retriever: Multi Query (Score: 0.963)\n",
            "\n",
            "ðŸ’° COST ANALYSIS:\n",
            "1. Compression: $0.0020\n",
            "2. Bm25: $0.0025\n",
            "3. Parent Doc: $0.0026\n",
            "4. Naive: $0.0036\n",
            "5. Multi Query: $0.0052\n",
            "6. Ensemble: $0.0071\n",
            "\n",
            "âš¡ LATENCY ANALYSIS:\n",
            "1. Bm25: 4.55s\n",
            "2. Parent Doc: 6.36s\n",
            "3. Naive: 6.65s\n",
            "4. Compression: 7.48s\n",
            "5. Multi Query: 8.79s\n",
            "6. Ensemble: 9.95s\n",
            "\n",
            "ï¿½ï¿½ LangSmith Dataset: https://smith.langchain.com/datasets/cfd03fa7-cbef-4680-acca-b7336570fc6e\n",
            "\n",
            "âœ… Evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5. DISPLAY RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for name, metrics in results.items():\n",
        "    comparison_data.append({\n",
        "        \"Retriever\": name.replace('_', ' ').title(),\n",
        "        \"Precision\": f\"{metrics['context_precision']:.3f}\",\n",
        "        \"Faithfulness\": f\"{metrics['faithfulness']:.3f}\",\n",
        "        \"Relevancy\": f\"{metrics['answer_relevancy']:.3f}\",\n",
        "        \"Recall\": f\"{metrics['context_recall']:.3f}\",\n",
        "        \"Latency (s)\": f\"{metrics['avg_latency']:.2f}\",\n",
        "        \"Cost ($)\": f\"{metrics['total_cost']:.4f}\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Calculate overall scores (weighted by performance metrics)\n",
        "for name, metrics in results.items():\n",
        "    overall = (metrics['context_precision'] * 0.3 + \n",
        "               metrics['faithfulness'] * 0.25 + \n",
        "               metrics['answer_relevancy'] * 0.25 + \n",
        "               metrics['context_recall'] * 0.2)\n",
        "    results[name]['overall'] = overall\n",
        "\n",
        "# Rank by performance\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['overall'], reverse=True)\n",
        "\n",
        "print(f\"\\nðŸ† RANKING:\")\n",
        "for i, (name, metrics) in enumerate(sorted_results):\n",
        "    print(f\"{i+1}. {name.replace('_', ' ').title()}: {metrics['overall']:.3f}\")\n",
        "\n",
        "best = sorted_results[0]\n",
        "print(f\"\\nðŸ’¡ Best retriever: {best[0].replace('_', ' ').title()} (Score: {best[1]['overall']:.3f})\")\n",
        "\n",
        "# Cost analysis\n",
        "print(f\"\\nðŸ’° COST ANALYSIS:\")\n",
        "cost_sorted = sorted(results.items(), key=lambda x: x[1]['total_cost'])\n",
        "for i, (name, metrics) in enumerate(cost_sorted):\n",
        "    print(f\"{i+1}. {name.replace('_', ' ').title()}: ${metrics['total_cost']:.4f}\")\n",
        "\n",
        "# Latency analysis\n",
        "print(f\"\\nâš¡ LATENCY ANALYSIS:\")\n",
        "latency_sorted = sorted(results.items(), key=lambda x: x[1]['avg_latency'])\n",
        "for i, (name, metrics) in enumerate(latency_sorted):\n",
        "    print(f\"{i+1}. {name.replace('_', ' ').title()}: {metrics['avg_latency']:.2f}s\")\n",
        "\n",
        "print(f\"\\nï¿½ï¿½ LangSmith Dataset: https://smith.langchain.com/datasets/{langsmith_dataset.id}\")\n",
        "print(\"\\nâœ… Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Summary: Advanced Retrieval Methods Evaluation\n",
        "\n",
        "This evaluation compared six different retrieval methods for RAG systems using loan complaint data. The analysis incorporated **performance metrics** (Ragas evaluation), **cost efficiency** (OpenAI API costs), and **latency** (response times) to provide a comprehensive comparison.\n",
        "\n",
        "### Performance Analysis\n",
        "\n",
        "#### ðŸ† Overall Performance Ranking\n",
        "1. **Multi Query** (0.963) - Highest overall score\n",
        "2. **Compression** (0.950) - Excellent performance\n",
        "3. **Parent Doc** (0.940) - Strong performance\n",
        "4. **Naive** (0.931) - Good baseline\n",
        "5. **BM25** (0.917) - Competitive performance\n",
        "6. **Ensemble** (0.910) - Lower than expected\n",
        "\n",
        "#### ðŸ“Š Key Performance Insights\n",
        "\n",
        "**Multi Query** emerged as the top performer with:\n",
        "- Perfect precision (1.000) and high faithfulness (0.940)\n",
        "- Best relevancy score (0.943) among all methods\n",
        "- Demonstrates the power of query reformulation for comprehensive retrieval\n",
        "\n",
        "**Compression** showed excellent results:\n",
        "- Perfect precision and recall (1.000)\n",
        "- Highest faithfulness score (0.972)\n",
        "- Proves reranking significantly improves answer quality\n",
        "\n",
        "**Parent Doc** achieved:\n",
        "- Perfect precision and recall (1.000)\n",
        "- Highest relevancy (0.940)\n",
        "- Shows that retrieving larger context chunks improves answer relevance\n",
        "\n",
        "### ðŸ’° Cost Analysis\n",
        "\n",
        "| Rank | Method      | Cost (\\$) | Highlights                                                   |\n",
        "| ---- | ----------- | --------- | ------------------------------------------------------------ |\n",
        "| 1    | Compression | 0.0020    | Best cost-performance ratio                                  |\n",
        "| 2    | BM25        | 0.0025    | Very cost-efficient                                          |\n",
        "| 3    | Parent Doc  | 0.0026    | Cost-effective                                               |\n",
        "| 4    | Naive       | 0.0036    | Moderate cost                                                |\n",
        "| 5    | Multi Query | 0.0052    | High cost due to multiple queries (2.6x cost of Compression) |\n",
        "| 6    | Ensemble    | 0.0071    | Most expensive, underperforms compared to others             |\n",
        "\n",
        "\n",
        "### âš¡ Latency Analysis\n",
        "\n",
        "| Rank | Method      | Latency (s) | Highlights                                    |\n",
        "| ---- | ----------- | ----------- | --------------------------------------------- |\n",
        "| 1    | BM25        | 4.55        | Fastest                                       |\n",
        "| 2    | Parent Doc  | 6.36        | Fast                                          |\n",
        "| 3    | Naive       | 6.65        | Moderate                                      |\n",
        "| 4    | Compression | 7.48        | Moderate                                      |\n",
        "| 5    | Multi Query | 8.79        | Slower due to multiple queries                |\n",
        "| 6    | Ensemble    | 9.95        | Slowest, multiple retrievers increase latency |\n",
        "\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "#### ðŸŽ¯ Best Overall Choice: **Compression**\n",
        "- **Why**: Excellent performance (0.950) with lowest cost ($0.0020)\n",
        "- **Trade-off**: Slightly slower (7.48s) but acceptable for most use cases\n",
        "- **Use case**: Production systems where cost and performance are equally important\n",
        "\n",
        "#### ðŸš€ Performance-First Choice: **Multi Query**\n",
        "- **Why**: Highest performance (0.963) with good precision\n",
        "- **Trade-off**: Higher cost ($0.0052) and slower (8.79s)\n",
        "- **Use case**: High-stakes applications where accuracy is paramount\n",
        "\n",
        "#### âš¡ Speed-First Choice: **BM25**\n",
        "- **Why**: Fastest (4.55s) with competitive performance (0.917)\n",
        "- **Trade-off**: Lower faithfulness score (0.919)\n",
        "- **Use case**: Real-time applications where speed is critical\n",
        "\n",
        "#### âŒ Avoid: **Ensemble**\n",
        "- **Why**: Highest cost ($0.0071) and slowest (9.95s) with lowest performance (0.910)\n",
        "- **Lesson**: More complex doesn't always mean better results\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Simplicity often wins**: BM25, a simple keyword-based method, performed competitively\n",
        "2. **Reranking is effective**: Compression's reranking significantly improved faithfulness\n",
        "3. **Query reformulation works**: Multi Query's approach of generating multiple queries improved recall\n",
        "4. **Context matters**: Parent Doc's larger context chunks improved relevancy\n",
        "5. **Cost-performance trade-offs**: The most expensive method (Ensemble) didn't provide the best results\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "For this loan complaint dataset, **Compression** offers the optimal balance of performance, cost, and speed. The evaluation demonstrates that advanced retrieval methods can significantly improve RAG system performance, but complexity doesn't always correlate with better results. The choice of retrieval method should be based on specific requirements for accuracy, speed, and cost constraints.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
