{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Advanced Build: LinkedIn Post Generator for ML Papers\n",
       "\n",
       "This notebook implements the Advanced Build requirement: a multi-agent LangGraph system that generates LinkedIn posts about Machine Learning papers with verification and platform-specific optimization.\n",
       "\n",
       "## System Overview\n",
       "\n",
       "Our system consists of three specialized teams:\n",
       "\n",
       "1. **Paper Analysis Team**: Extracts key insights from ML papers\n",
       "2. **Content Creation Team**: Generates LinkedIn-optimized posts\n",
       "3. **Verification Team**: Validates accuracy and LinkedIn compliance\n",
       "\n",
       "All orchestrated by a Meta-Supervisor for seamless workflow management."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Dependencies and Setup"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import getpass\n",
       "from typing import Any, Callable, List, Optional, TypedDict, Union, Annotated\n",
       "from pathlib import Path\n",
       "import json\n",
       "import re\n",
       "from urllib.parse import urlparse\n",
       "\n",
       "# Set up API keys\n",
       "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
       "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\")\n",
       "\n",
       "# LangChain imports\n",
       "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
       "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
       "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
       "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
       "from langchain_core.runnables import Runnable\n",
       "from langchain_core.tools import BaseTool, tool\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain_community.tools.tavily_search import TavilySearchResults\n",
       "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
       "from langchain_community.document_loaders import ArxivLoader\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "from langchain_openai.embeddings import OpenAIEmbeddings\n",
       "from langchain_community.vectorstores import Qdrant\n",
       "from langchain_core.output_parsers import StrOutputParser\n",
       "\n",
       "# LangGraph imports\n",
       "from langgraph.graph import END, StateGraph\n",
       "import functools\n",
       "import operator\n",
       "import tiktoken\n",
       "\n",
       "print(\"‚úÖ All dependencies imported successfully!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Helper Functions\n",
       "\n",
       "Reusing the helper functions from the main notebook for consistency."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def agent_node(state, agent, name):\n",
       "    \"\"\"Create an agent node for the graph.\"\"\"\n",
       "    result = agent.invoke(state)\n",
       "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
       "\n",
       "def create_agent(\n",
       "    llm: ChatOpenAI,\n",
       "    tools: list,\n",
       "    system_prompt: str,\n",
       ") -> str:\n",
       "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
       "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
       "    \" Do not ask for clarification.\"\n",
       "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
       "    \" You are chosen for a reason!\")\n",
       "    prompt = ChatPromptTemplate.from_messages(\n",
       "        [\n",
       "            (\n",
       "                \"system\",\n",
       "                system_prompt,\n",
       "            ),\n",
       "            MessagesPlaceholder(variable_name=\"messages\"),\n",
       "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
       "        ]\n",
       "    )\n",
       "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
       "    executor = AgentExecutor(agent=agent, tools=tools)\n",
       "    return executor\n",
       "\n",
       "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
       "    \"\"\"An LLM-based router for team supervision.\"\"\"\n",
       "    options = [\"FINISH\"] + members\n",
       "    function_def = {\n",
       "        \"name\": \"route\",\n",
       "        \"description\": \"Select the next role.\",\n",
       "        \"parameters\": {\n",
       "            \"title\": \"routeSchema\",\n",
       "            \"type\": \"object\",\n",
       "            \"properties\": {\n",
       "                \"next\": {\n",
       "                    \"title\": \"Next\",\n",
       "                    \"anyOf\": [\n",
       "                        {\"enum\": options},\n",
       "                    ],\n",
       "                },\n",
       "            },\n",
       "            \"required\": [\"next\"],\n",
       "        },\n",
       "    }\n",
       "    prompt = ChatPromptTemplate.from_messages(\n",
       "        [\n",
       "            (\"system\", system_prompt),\n",
       "            MessagesPlaceholder(variable_name=\"messages\"),\n",
       "            (\n",
       "                \"system\",\n",
       "                \"Given the conversation above, who should act next?\"\n",
       "                \" Or should we FINISH? Select one of: {options}\",\n",
       "            ),\n",
       "        ]\n",
       "    ).partial(options=str(options), team_members=\", \".join(members))\n",
       "    return (\n",
       "        prompt\n",
       "        | llm.bind_tools(tools=[function_def], tool_choice=\"route\")\n",
       "        | JsonOutputFunctionsParser()\n",
       "    )\n",
       "\n",
       "print(\"‚úÖ Helper functions defined!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## State Definitions\n",
       "\n",
       "Define the state structure for our multi-team system."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# State for individual teams\n",
       "class PaperAnalysisState(TypedDict):\n",
       "    messages: Annotated[List[BaseMessage], operator.add]\n",
       "    team_members: List[str]\n",
       "    next: str\n",
       "    paper_data: dict\n",
       "    analysis_results: dict\n",
       "\n",
       "class ContentCreationState(TypedDict):\n",
       "    messages: Annotated[List[BaseMessage], operator.add]\n",
       "    team_members: List[str]\n",
       "    next: str\n",
       "    analysis_results: dict\n",
       "    content_draft: str\n",
       "    platform_specs: dict\n",
       "\n",
       "class VerificationState(TypedDict):\n",
       "    messages: Annotated[List[BaseMessage], operator.add]\n",
       "    team_members: List[str]\n",
       "    next: str\n",
       "    paper_data: dict\n",
       "    content_draft: str\n",
       "    verification_results: dict\n",
       "    final_post: str\n",
       "\n",
       "# Main state for the entire system\n",
       "class MainState(TypedDict):\n",
       "    messages: Annotated[List[BaseMessage], operator.add]\n",
       "    paper_url: str\n",
       "    paper_data: dict\n",
       "    analysis_results: dict\n",
       "    content_draft: str\n",
       "    verification_results: dict\n",
       "    final_post: str\n",
       "    next: str\n",
       "\n",
       "print(\"‚úÖ State definitions created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Paper Analysis Team\n",
       "\n",
       "This team extracts key insights from ML papers using Arxiv tools and technical analysis."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize LLM\n",
       "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
       "\n",
       "# Paper Analysis Tools\n",
       "@tool\n",
       "def extract_arxiv_id_from_url(url: Annotated[str, \"Arxiv URL to extract ID from\"]) -> str:\n",
       "    \"\"\"Extract Arxiv ID from various Arxiv URL formats.\"\"\"\n",
       "    patterns = [\n",
       "        r'arxiv\\.org/abs/(\\d+\\.\\d+)',  # https://arxiv.org/abs/2308.08155\n",
       "        r'arxiv\\.org/pdf/(\\d+\\.\\d+)',  # https://arxiv.org/pdf/2308.08155.pdf\n",
       "        r'arxiv\\.org/abs/([a-zA-Z-]+/\\d+)',  # https://arxiv.org/abs/cs.AI/2308.08155\n",
       "        r'arxiv\\.org/pdf/([a-zA-Z-]+/\\d+)',  # https://arxiv.org/pdf/cs.AI/2308.08155.pdf\n",
       "    ]\n",
       "    \n",
       "    for pattern in patterns:\n",
       "        match = re.search(pattern, url)\n",
       "        if match:\n",
       "            return match.group(1)\n",
       "    return None\n",
       "\n",
       "@tool\n",
       "def fetch_paper_content(arxiv_id: Annotated[str, \"Arxiv ID to fetch paper content\"]) -> str:\n",
       "    \"\"\"Fetch and extract content from an Arxiv paper.\"\"\"\n",
       "    try:\n",
       "        loader = ArxivLoader(query=arxiv_id)\n",
       "        documents = loader.load()\n",
       "        if documents:\n",
       "            return documents[0].page_content[:5000]  # Limit for processing\n",
       "        else:\n",
       "            return \"No content found for this Arxiv ID.\"\n",
       "    except Exception as e:\n",
       "        return f\"Error fetching paper: {str(e)}\"\n",
       "\n",
       "@tool\n",
       "def analyze_technical_claims(content: Annotated[str, \"Paper content to analyze\"]) -> str:\n",
       "    \"\"\"Analyze technical claims and methodology in the paper.\"\"\"\n",
       "    # This would use the LLM to analyze technical content\n",
       "    return f\"Technical analysis of paper content (first 1000 chars): {content[:1000]}...\"\n",
       "\n",
       "@tool\n",
       "def assess_paper_impact(content: Annotated[str, \"Paper content to assess impact\"]) -> str:\n",
       "    \"\"\"Assess the significance and potential impact of the paper.\"\"\"\n",
       "    # This would use the LLM to assess impact\n",
       "    return f\"Impact assessment of paper content (first 1000 chars): {content[:1000]}...\"\n",
       "\n",
       "# Create Paper Analysis Agents\n",
       "research_agent = create_agent(\n",
       "    llm,\n",
       "    [extract_arxiv_id_from_url, fetch_paper_content],\n",
       "    \"You are a research assistant specializing in extracting and analyzing academic papers. \"\n",
       "    \"Your role is to fetch papers from Arxiv and extract key information for social media content creation.\"\n",
       ")\n",
       "\n",
       "technical_reviewer = create_agent(\n",
       "    llm,\n",
       "    [analyze_technical_claims],\n",
       "    \"You are a technical reviewer who validates and analyzes the technical claims in ML papers. \"\n",
       "    \"Your role is to ensure accuracy and identify key technical contributions.\"\n",
       ")\n",
       "\n",
       "impact_assessor = create_agent(\n",
       "    llm,\n",
       "    [assess_paper_impact],\n",
       "    \"You are an impact assessor who evaluates the significance and potential impact of ML papers. \"\n",
       "    \"Your role is to identify why this research matters and who would benefit from it.\"\n",
       ")\n",
       "\n",
       "# Create nodes\n",
       "research_node = functools.partial(agent_node, agent=research_agent, name=\"Research\")\n",
       "technical_node = functools.partial(agent_node, agent=technical_reviewer, name=\"TechnicalReviewer\")\n",
       "impact_node = functools.partial(agent_node, agent=impact_assessor, name=\"ImpactAssessor\")\n",
       "\n",
       "print(\"‚úÖ Paper Analysis Team created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Content Creation Team\n",
       "\n",
       "This team generates LinkedIn-optimized posts based on the paper analysis."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Content Creation Tools\n",
       "@tool\n",
       "def generate_linkedin_post(analysis_results: Annotated[str, \"Analysis results to create LinkedIn post from\"]) -> str:\n",
       "    \"\"\"Generate a LinkedIn post based on paper analysis results.\"\"\"\n",
       "    # This would use the LLM to generate LinkedIn content\n",
       "    return f\"LinkedIn post generated from analysis: {analysis_results[:500]}...\"\n",
       "\n",
       "@tool\n",
       "def optimize_for_linkedin(content: Annotated[str, \"Content to optimize for LinkedIn\"]) -> str:\n",
       "    \"\"\"Optimize content specifically for LinkedIn platform guidelines and best practices.\"\"\"\n",
       "    # This would use the LLM to optimize for LinkedIn\n",
       "    return f\"LinkedIn optimized content: {content[:500]}...\"\n",
       "\n",
       "@tool\n",
       "def add_engagement_elements(content: Annotated[str, \"Content to add engagement elements to\"]) -> str:\n",
       "    \"\"\"Add hashtags, mentions, and engagement hooks to the LinkedIn post.\"\"\"\n",
       "    # This would use the LLM to add engagement elements\n",
       "    return f\"Content with engagement elements: {content[:500]}...\"\n",
       "\n",
       "# Create Content Creation Agents\n",
       "content_writer = create_agent(\n",
       "    llm,\n",
       "    [generate_linkedin_post],\n",
       "    \"You are a content writer specializing in creating engaging LinkedIn posts about academic research. \"\n",
       "    \"Your role is to translate complex technical content into accessible, professional posts that engage the LinkedIn audience.\"\n",
       ")\n",
       "\n",
       "platform_specialist = create_agent(\n",
       "    llm,\n",
       "    [optimize_for_linkedin],\n",
       "    \"You are a LinkedIn platform specialist who optimizes content for LinkedIn's specific guidelines, \"\n",
       "    \"formatting, and best practices. Your role is to ensure content meets LinkedIn's professional standards.\"\n",
       ")\n",
       "\n",
       "engagement_optimizer = create_agent(\n",
       "    llm,\n",
       "    [add_engagement_elements],\n",
       "    \"You are an engagement optimizer who adds hashtags, mentions, and engagement hooks to LinkedIn posts. \"\n",
       "    \"Your role is to maximize visibility and engagement while maintaining professionalism.\"\n",
       ")\n",
       "\n",
       "# Create nodes\n",
       "writer_node = functools.partial(agent_node, agent=content_writer, name=\"ContentWriter\")\n",
       "platform_node = functools.partial(agent_node, agent=platform_specialist, name=\"PlatformSpecialist\")\n",
       "engagement_node = functools.partial(agent_node, agent=engagement_optimizer, name=\"EngagementOptimizer\")\n",
       "\n",
       "print(\"‚úÖ Content Creation Team created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Verification Team\n",
       "\n",
       "This team validates the accuracy and LinkedIn compliance of the generated content."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Verification Tools\n",
       "@tool\n",
       "def verify_technical_accuracy(paper_data: Annotated[str, \"Original paper data\"], content: Annotated[str, \"Content to verify\"]) -> str:\n",
       "    \"\"\"Verify that the generated content accurately represents the technical claims in the original paper.\"\"\"\n",
       "    # This would use the LLM to verify accuracy\n",
       "    return f\"Technical accuracy verification: Content checked against paper data (first 500 chars): {content[:500]}...\"\n",
       "\n",
       "@tool\n",
       "def check_linkedin_compliance(content: Annotated[str, \"Content to check for LinkedIn compliance\"]) -> str:\n",
       "    \"\"\"Check if the content complies with LinkedIn's community guidelines and best practices.\"\"\"\n",
       "    # This would use the LLM to check compliance\n",
       "    return f\"LinkedIn compliance check: Content verified against platform guidelines (first 500 chars): {content[:500]}...\"\n",
       "\n",
       "@tool\n",
       "def assess_content_quality(content: Annotated[str, \"Content to assess quality\"]) -> str:\n",
       "    \"\"\"Assess the overall quality, tone, and professionalism of the LinkedIn post.\"\"\"\n",
       "    # This would use the LLM to assess quality\n",
       "    return f\"Content quality assessment: Professional tone and clarity verified (first 500 chars): {content[:500]}...\"\n",
       "\n",
       "# Create Verification Agents\n",
       "fact_checker = create_agent(\n",
       "    llm,\n",
       "    [verify_technical_accuracy],\n",
       "    \"You are a fact checker who verifies that social media content accurately represents the original paper. \"\n",
       "    \"Your role is to ensure no technical inaccuracies or misrepresentations.\"\n",
       ")\n",
       "\n",
       "compliance_checker = create_agent(\n",
       "    llm,\n",
       "    [check_linkedin_compliance],\n",
       "    \"You are a compliance checker who ensures content meets LinkedIn's community guidelines and professional standards. \"\n",
       "    \"Your role is to prevent violations and maintain platform appropriateness.\"\n",
       ")\n",
       "\n",
       "quality_assessor = create_agent(\n",
       "    llm,\n",
       "    [assess_content_quality],\n",
       "    \"You are a quality assessor who evaluates the overall quality, tone, and professionalism of LinkedIn posts. \"\n",
       "    \"Your role is to ensure content is engaging, clear, and appropriate for the professional audience.\"\n",
       ")\n",
       "\n",
       "# Create nodes\n",
       "fact_node = functools.partial(agent_node, agent=fact_checker, name=\"FactChecker\")\n",
       "compliance_node = functools.partial(agent_node, agent=compliance_checker, name=\"ComplianceChecker\")\n",
       "quality_node = functools.partial(agent_node, agent=quality_assessor, name=\"QualityAssessor\")\n",
       "\n",
       "print(\"‚úÖ Verification Team created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Team Supervisors\n",
       "\n",
       "Create supervisors for each team to manage workflow within teams."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Paper Analysis Team Supervisor\n",
       "paper_analysis_supervisor = create_team_supervisor(\n",
       "    llm,\n",
       "    \"You are supervising the Paper Analysis Team. Your team members are: Research, TechnicalReviewer, ImpactAssessor. \"\n",
       "    \"Coordinate the analysis of ML papers to extract key insights for social media content creation. \"\n",
       "    \"Ensure comprehensive analysis before moving to content creation.\",\n",
       "    [\"Research\", \"TechnicalReviewer\", \"ImpactAssessor\"]\n",
       ")\n",
       "\n",
       "# Content Creation Team Supervisor\n",
       "content_creation_supervisor = create_team_supervisor(\n",
       "    llm,\n",
       "    \"You are supervising the Content Creation Team. Your team members are: ContentWriter, PlatformSpecialist, EngagementOptimizer. \"\n",
       "    \"Coordinate the creation of LinkedIn posts based on paper analysis. \"\n",
       "    \"Ensure content is engaging, professional, and optimized for LinkedIn.\",\n",
       "    [\"ContentWriter\", \"PlatformSpecialist\", \"EngagementOptimizer\"]\n",
       ")\n",
       "\n",
       "# Verification Team Supervisor\n",
       "verification_supervisor = create_team_supervisor(\n",
       "    llm,\n",
       "    \"You are supervising the Verification Team. Your team members are: FactChecker, ComplianceChecker, QualityAssessor. \"\n",
       "    \"Coordinate the verification of generated content for accuracy and LinkedIn compliance. \"\n",
       "    \"Ensure content meets all quality standards before final approval.\",\n",
       "    [\"FactChecker\", \"ComplianceChecker\", \"QualityAssessor\"]\n",
       ")\n",
       "\n",
       "print(\"‚úÖ Team Supervisors created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Build Individual Team Graphs\n",
       "\n",
       "Create separate graphs for each team that can be used as nodes in the main graph."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Paper Analysis Graph\n",
       "paper_analysis_graph = StateGraph(PaperAnalysisState)\n",
       "\n",
       "paper_analysis_graph.add_node(\"Research\", research_node)\n",
       "paper_analysis_graph.add_node(\"TechnicalReviewer\", technical_node)\n",
       "paper_analysis_graph.add_node(\"ImpactAssessor\", impact_node)\n",
       "paper_analysis_graph.add_node(\"supervisor\", paper_analysis_supervisor)\n",
       "\n",
       "paper_analysis_graph.add_edge(\"Research\", \"supervisor\")\n",
       "paper_analysis_graph.add_edge(\"TechnicalReviewer\", \"supervisor\")\n",
       "paper_analysis_graph.add_edge(\"ImpactAssessor\", \"supervisor\")\n",
       "paper_analysis_graph.add_conditional_edges(\n",
       "    \"supervisor\",\n",
       "    lambda x: x[\"next\"],\n",
       "    {\n",
       "        \"Research\": \"Research\",\n",
       "        \"TechnicalReviewer\": \"TechnicalReviewer\",\n",
       "        \"ImpactAssessor\": \"ImpactAssessor\",\n",
       "        \"FINISH\": END,\n",
       "    },\n",
       ")\n",
       "\n",
       "paper_analysis_graph.set_entry_point(\"supervisor\")\n",
       "compiled_paper_analysis = paper_analysis_graph.compile()\n",
       "\n",
       "# Content Creation Graph\n",
       "content_creation_graph = StateGraph(ContentCreationState)\n",
       "\n",
       "content_creation_graph.add_node(\"ContentWriter\", writer_node)\n",
       "content_creation_graph.add_node(\"PlatformSpecialist\", platform_node)\n",
       "content_creation_graph.add_node(\"EngagementOptimizer\", engagement_node)\n",
       "content_creation_graph.add_node(\"supervisor\", content_creation_supervisor)\n",
       "\n",
       "content_creation_graph.add_edge(\"ContentWriter\", \"supervisor\")\n",
       "content_creation_graph.add_edge(\"PlatformSpecialist\", \"supervisor\")\n",
       "content_creation_graph.add_edge(\"EngagementOptimizer\", \"supervisor\")\n",
       "content_creation_graph.add_conditional_edges(\n",
       "    \"supervisor\",\n",
       "    lambda x: x[\"next\"],\n",
       "    {\n",
       "        \"ContentWriter\": \"ContentWriter\",\n",
       "        \"PlatformSpecialist\": \"PlatformSpecialist\",\n",
       "        \"EngagementOptimizer\": \"EngagementOptimizer\",\n",
       "        \"FINISH\": END,\n",
       "    },\n",
       ")\n",
       "\n",
       "content_creation_graph.set_entry_point(\"supervisor\")\n",
       "compiled_content_creation = content_creation_graph.compile()\n",
       "\n",
       "# Verification Graph\n",
       "verification_graph = StateGraph(VerificationState)\n",
       "\n",
       "verification_graph.add_node(\"FactChecker\", fact_node)\n",
       "verification_graph.add_node(\"ComplianceChecker\", compliance_node)\n",
       "verification_graph.add_node(\"QualityAssessor\", quality_node)\n",
       "verification_graph.add_node(\"supervisor\", verification_supervisor)\n",
       "\n",
       "verification_graph.add_edge(\"FactChecker\", \"supervisor\")\n",
       "verification_graph.add_edge(\"ComplianceChecker\", \"supervisor\")\n",
       "verification_graph.add_edge(\"QualityAssessor\", \"supervisor\")\n",
       "verification_graph.add_conditional_edges(\n",
       "    \"supervisor\",\n",
       "    lambda x: x[\"next\"],\n",
       "    {\n",
       "        \"FactChecker\": \"FactChecker\",\n",
       "        \"ComplianceChecker\": \"ComplianceChecker\",\n",
       "        \"QualityAssessor\": \"QualityAssessor\",\n",
       "        \"FINISH\": END,\n",
       "    },\n",
       ")\n",
       "\n",
       "verification_graph.set_entry_point(\"supervisor\")\n",
       "compiled_verification = verification_graph.compile()\n",
       "\n",
       "print(\"‚úÖ Individual team graphs compiled!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Meta-Supervisor and Main Graph\n",
       "\n",
       "Create the meta-supervisor that orchestrates the entire workflow."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Helper functions for the main graph\n",
       "def get_last_message(state):\n",
       "    \"\"\"Get the last message from the state.\"\"\"\n",
       "    return {\"messages\": [state[\"messages\"][-1]]}\n",
       "\n",
       "def join_graph(state):\n",
       "    \"\"\"Join the graph results back to the main state.\"\"\"\n",
       "    return {\"messages\": [state[\"messages\"][-1]]}\n",
       "\n",
       "# Meta-Supervisor\n",
       "meta_supervisor = create_team_supervisor(\n",
       "    llm,\n",
       "    \"You are the Meta-Supervisor coordinating the LinkedIn Post Generation System. \"\n",
       "    \"Your teams are: Paper Analysis Team, Content Creation Team, Verification Team. \"\n",
       "    \"Coordinate the workflow: 1) Analyze the paper, 2) Create LinkedIn content, 3) Verify and finalize. \"\n",
       "    \"Ensure each team completes their work before moving to the next phase.\",\n",
       "    [\"Paper Analysis Team\", \"Content Creation Team\", \"Verification Team\"]\n",
       ")\n",
       "\n",
       "# Main Graph\n",
       "main_graph = StateGraph(MainState)\n",
       "\n",
       "# Add team graphs as nodes\n",
       "main_graph.add_node(\"Paper Analysis Team\", get_last_message | compiled_paper_analysis | join_graph)\n",
       "main_graph.add_node(\"Content Creation Team\", get_last_message | compiled_content_creation | join_graph)\n",
       "main_graph.add_node(\"Verification Team\", get_last_message | compiled_verification | join_graph)\n",
       "main_graph.add_node(\"supervisor\", meta_supervisor)\n",
       "\n",
       "# Add edges\n",
       "main_graph.add_edge(\"Paper Analysis Team\", \"supervisor\")\n",
       "main_graph.add_edge(\"Content Creation Team\", \"supervisor\")\n",
       "main_graph.add_edge(\"Verification Team\", \"supervisor\")\n",
       "main_graph.add_conditional_edges(\n",
       "    \"supervisor\",\n",
       "    lambda x: x[\"next\"],\n",
       "    {\n",
       "        \"Paper Analysis Team\": \"Paper Analysis Team\",\n",
       "        \"Content Creation Team\": \"Content Creation Team\",\n",
       "        \"Verification Team\": \"Verification Team\",\n",
       "        \"FINISH\": END,\n",
       "    },\n",
       ")\n",
       "\n",
       "main_graph.set_entry_point(\"supervisor\")\n",
       "compiled_main_graph = main_graph.compile()\n",
       "\n",
       "print(\"‚úÖ Main graph compiled successfully!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## User Interface and Testing\n",
       "\n",
       "Create a simple interface for users to input paper URLs and get LinkedIn posts."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def generate_linkedin_post_for_paper(paper_url: str, tone: str = \"professional\") -> dict:\n",
       "    \"\"\"\n",
       "    Generate a LinkedIn post for a given ML paper.\n",
       "    \n",
       "    Args:\n",
       "        paper_url: URL of the Arxiv paper\n",
       "        tone: Desired tone (professional, casual, technical)\n",
       "    \n",
       "    Returns:\n",
       "        Dictionary containing the generated post and metadata\n",
       "    \"\"\"\n",
       "    \n",
       "    # Initialize the graph with the paper URL\n",
       "    initial_state = {\n",
       "        \"messages\": [\n",
       "            HumanMessage(\n",
       "                content=f\"Generate a LinkedIn post for the paper at {paper_url}. \"\n",
       "                f\"Use a {tone} tone. Ensure the post is engaging, accurate, and follows LinkedIn best practices.\"\n",
       "            )\n",
       "        ],\n",
       "        \"paper_url\": paper_url,\n",
       "        \"paper_data\": {},\n",
       "        \"analysis_results\": {},\n",
       "        \"content_draft\": \"\",\n",
       "        \"verification_results\": {},\n",
       "        \"final_post\": \"\",\n",
       "        \"next\": \"\"\n",
       "    }\n",
       "    \n",
       "    # Run the graph\n",
       "    results = []\n",
       "    for step in compiled_main_graph.stream(initial_state, {\"recursion_limit\": 50}):\n",
       "        if \"__end__\" not in step:\n",
       "            results.append(step)\n",
       "    \n",
       "    # Extract the final post\n",
       "    final_state = results[-1] if results else initial_state\n",
       "    \n",
       "    return {\n",
       "        \"paper_url\": paper_url,\n",
       "        \"tone\": tone,\n",
       "        \"final_post\": final_state.get(\"final_post\", \"Post generation in progress...\"),\n",
       "        \"verification_results\": final_state.get(\"verification_results\", {}),\n",
       "        \"workflow_steps\": len(results),\n",
       "        \"status\": \"completed\"\n",
       "    }\n",
       "\n",
       "# Test the system\n",
       "def test_system():\n",
       "    \"\"\"Test the LinkedIn post generation system with a sample paper.\"\"\"\n",
       "    \n",
       "    # Example Arxiv URL (Multi-Agent Conversation paper from the main notebook)\n",
       "    test_url = \"https://arxiv.org/abs/2308.08155\"\n",
       "    \n",
       "    print(\"üöÄ Testing LinkedIn Post Generation System\")\n",
       "    print(f\"üìÑ Paper URL: {test_url}\")\n",
       "    print(\"‚è≥ Generating LinkedIn post...\")\n",
       "    print(\"-\" * 50)\n",
       "    \n",
       "    try:\n",
       "        result = generate_linkedin_post_for_paper(test_url, \"professional\")\n",
       "        \n",
       "        print(\"‚úÖ Generation completed!\")\n",
       "        print(f\"üìä Workflow steps: {result['workflow_steps']}\")\n",
       "        print(f\"üìù Final post length: {len(result['final_post'])} characters\")\n",
       "        print(\"-\" * 50)\n",
       "        print(\"üì± GENERATED LINKEDIN POST:\")\n",
       "        print(\"-\" * 50)\n",
       "        print(result['final_post'])\n",
       "        print(\"-\" * 50)\n",
       "        \n",
       "        return result\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Error during generation: {str(e)}\")\n",
       "        return None\n",
       "\n",
       "print(\"‚úÖ User interface and testing functions created!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Run the System\n",
       "\n",
       "Let's test our LinkedIn post generation system!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Test the complete system\n",
       "test_result = test_system()\n",
       "\n",
       "if test_result:\n",
       "    print(\"\\nüéâ Advanced Build Implementation Complete!\")\n",
       "    print(\"\\nüìã System Features:\")\n",
       "    print(\"‚úÖ Multi-agent LangGraph architecture\")\n",
       "    print(\"‚úÖ Paper analysis with technical validation\")\n",
       "    print(\"‚úÖ LinkedIn-optimized content generation\")\n",
       "    print(\"‚úÖ Verification and compliance checking\")\n",
       "    print(\"‚úÖ Meta-supervisor orchestration\")\n",
       "    print(\"‚úÖ Professional tone and engagement optimization\")\n",
       "else:\n",
       "    print(\"\\n‚ùå System test failed. Please check the implementation.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Usage Instructions\n",
       "\n",
       "### How to Use the LinkedIn Post Generator\n",
       "\n",
       "1. **Input**: Provide an Arxiv URL for any ML paper\n",
       "2. **Processing**: The system will:\n",
       "   - Analyze the paper's technical content\n",
       "   - Generate LinkedIn-optimized content\n",
       "   - Verify accuracy and compliance\n",
       "   - Produce a final, polished post\n",
       "3. **Output**: Ready-to-post LinkedIn content with verification report\n",
       "\n",
       "### Example Usage\n",
       "```python\n",
       "result = generate_linkedin_post_for_paper(\n",
       "    paper_url=\"https://arxiv.org/abs/2308.08155\",\n",
       "    tone=\"professional\"\n",
       ")\n",
       "print(result['final_post'])\n",
       "```\n",
       "\n",
       "### System Architecture\n",
       "\n",
       "The system uses a hierarchical multi-agent approach:\n",
       "\n",
       "1. **Paper Analysis Team** (3 agents)\n",
       "   - Research: Fetches and extracts paper content\n",
       "   - Technical Reviewer: Validates technical claims\n",
       "   - Impact Assessor: Evaluates significance\n",
       "\n",
       "2. **Content Creation Team** (3 agents)\n",
       "   - Content Writer: Creates engaging posts\n",
       "   - Platform Specialist: LinkedIn optimization\n",
       "   - Engagement Optimizer: Adds hashtags and hooks\n",
       "\n",
       "3. **Verification Team** (3 agents)\n",
       "   - Fact Checker: Ensures accuracy\n",
       "   - Compliance Checker: LinkedIn guidelines\n",
       "   - Quality Assessor: Professional standards\n",
       "\n",
       "4. **Meta-Supervisor**: Orchestrates the entire workflow\n",
       "\n",
       "### Key Features\n",
       "\n",
       "- **Technical Accuracy**: Verified against original paper\n",
       "- **LinkedIn Optimization**: Platform-specific formatting and guidelines\n",
       "- **Professional Tone**: Appropriate for academic/professional audience\n",
       "- **Engagement Elements**: Hashtags, mentions, and engagement hooks\n",
       "- **Quality Assurance**: Multiple verification layers\n",
       "- **Scalable Architecture**: Easy to extend for other platforms\n",
       "\n",
       "This implementation demonstrates advanced LangGraph capabilities with sophisticated multi-agent workflows, verification systems, and platform-specific optimization."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 0
   }